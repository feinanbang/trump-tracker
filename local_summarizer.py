#!/usr/bin/env python3
"""
æœ¬åœ°AIå°ç»“ç”Ÿæˆå™¨
ä¸ä¾èµ–å¤–éƒ¨APIï¼Œä½¿ç”¨æœ¬åœ°ç®—æ³•ç”Ÿæˆæ™ºèƒ½å°ç»“
"""

import re
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import pytz

from database import TrumpPostsDB
from config import TIMEZONE

logger = logging.getLogger(__name__)


class LocalTrumpSummarizer:
    """æœ¬åœ°Trumpå¸–å­æ™ºèƒ½å°ç»“ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.db = TrumpPostsDB()
        self.et_tz = pytz.timezone(TIMEZONE)
        
        # æ”¿æ²»å…³é”®è¯è¯å…¸
        self.political_keywords = {
            'election': ['election', 'vote', 'voting', 'ballot', 'poll', 'æ°‘è°ƒ', 'é€‰ä¸¾', 'æŠ•ç¥¨'],
            'economy': ['economy', 'economic', 'jobs', 'employment', 'business', 'ç»æµ', 'å°±ä¸š', 'å•†ä¸š'],
            'border': ['border', 'immigration', 'wall', 'security', 'è¾¹å¢ƒ', 'ç§»æ°‘', 'å®‰å…¨'],
            'media': ['media', 'fake news', 'press', 'journalist', 'åª’ä½“', 'æ–°é—»', 'è®°è€…'],
            'political': ['democrat', 'republican', 'congress', 'senate', 'æ°‘ä¸»å…š', 'å…±å’Œå…š', 'å›½ä¼š'],
            'legal': ['court', 'judge', 'trial', 'case', 'legal', 'æ³•é™¢', 'æ³•å®˜', 'å®¡åˆ¤'],
            'campaign': ['campaign', 'rally', 'support', 'donate', 'ç«é€‰', 'é›†ä¼š', 'æ”¯æŒ']
        }
        
        # æƒ…æ„Ÿè¯å…¸
        self.sentiment_words = {
            'positive': ['great', 'excellent', 'wonderful', 'amazing', 'fantastic', 'best', 'winning'],
            'negative': ['terrible', 'horrible', 'worst', 'bad', 'failing', 'disaster', 'corrupt'],
            'neutral': ['said', 'reported', 'announced', 'stated', 'mentioned']
        }
        
    def extract_keywords(self, posts: List[Dict]) -> Dict[str, int]:
        """æå–å…³é”®è¯å’Œé¢‘ç‡"""
        all_text = ' '.join([post.get('content', '') for post in posts]).lower()
        
        keyword_counts = {}
        for category, keywords in self.political_keywords.items():
            count = 0
            for keyword in keywords:
                count += len(re.findall(r'\b' + re.escape(keyword.lower()) + r'\b', all_text))
            if count > 0:
                keyword_counts[category] = count
        
        return keyword_counts
    
    def analyze_sentiment(self, posts: List[Dict]) -> str:
        """åˆ†ææƒ…æ„Ÿå€¾å‘"""
        all_text = ' '.join([post.get('content', '') for post in posts]).lower()
        
        sentiment_scores = {}
        for sentiment, words in self.sentiment_words.items():
            score = 0
            for word in words:
                score += len(re.findall(r'\b' + re.escape(word) + r'\b', all_text))
            sentiment_scores[sentiment] = score
        
        if sentiment_scores['positive'] > sentiment_scores['negative']:
            return "ç§¯æä¹è§‚"
        elif sentiment_scores['negative'] > sentiment_scores['positive']:
            return "æ‰¹è¯„è´¨ç–‘"
        else:
            return "ä¸­æ€§é™ˆè¿°"
    
    def extract_mentions(self, posts: List[Dict]) -> List[str]:
        """æå–@æåŠå’Œé‡è¦äººç‰©"""
        mentions = set()
        all_text = ' '.join([post.get('content', '') for post in posts])
        
        # æå–@mentions
        at_mentions = re.findall(r'@(\w+)', all_text)
        mentions.update(at_mentions)
        
        # é‡è¦äººç‰©å…³é”®è¯
        important_people = ['Biden', 'Harris', 'Pelosi', 'McCarthy', 'DeSantis', 'æ‹œç™»', 'å“ˆé‡Œæ–¯']
        for person in important_people:
            if person.lower() in all_text.lower():
                mentions.add(person)
        
        return list(mentions)[:5]  # æœ€å¤šè¿”å›5ä¸ª
    
    def categorize_content(self, posts: List[Dict]) -> Dict[str, List[str]]:
        """å¯¹å†…å®¹è¿›è¡Œåˆ†ç±»"""
        categories = {
            'æ”¿æ²»è§‚ç‚¹': [],
            'åª’ä½“æ‰¹è¯„': [],
            'æ”¿ç­–ä¸»å¼ ': [],
            'ä¸ªäººç”Ÿæ´»': [],
            'å…¶ä»–': []
        }
        
        for post in posts:
            content = post.get('content', '').lower()
            post_time = post.get('post_time', '')
            short_content = content[:50] + "..." if len(content) > 50 else content
            
            if any(word in content for word in ['fake news', 'media', 'press', 'åª’ä½“', 'æ–°é—»']):
                categories['åª’ä½“æ‰¹è¯„'].append(f"[{post_time}] {short_content}")
            elif any(word in content for word in ['policy', 'border', 'economy', 'æ”¿ç­–', 'è¾¹å¢ƒ', 'ç»æµ']):
                categories['æ”¿ç­–ä¸»å¼ '].append(f"[{post_time}] {short_content}")
            elif any(word in content for word in ['election', 'vote', 'campaign', 'é€‰ä¸¾', 'ç«é€‰']):
                categories['æ”¿æ²»è§‚ç‚¹'].append(f"[{post_time}] {short_content}")
            elif any(word in content for word in ['family', 'personal', 'golf', 'å®¶åº­', 'ä¸ªäºº']):
                categories['ä¸ªäººç”Ÿæ´»'].append(f"[{post_time}] {short_content}")
            else:
                categories['å…¶ä»–'].append(f"[{post_time}] {short_content}")
        
        # ç§»é™¤ç©ºåˆ†ç±»
        return {k: v for k, v in categories.items() if v}
    
    def generate_time_analysis(self, posts: List[Dict]) -> str:
        """åˆ†æå‘å¸–æ—¶é—´æ¨¡å¼"""
        times = []
        for post in posts:
            time_str = post.get('post_time', '')
            if time_str:
                try:
                    hour = int(time_str.split(':')[0])
                    times.append(hour)
                except:
                    continue
        
        if not times:
            return "æ—¶é—´åˆ†å¸ƒï¼šæ— æ³•åˆ†æ"
        
        if max(times) - min(times) <= 2:
            return f"æ—¶é—´åˆ†å¸ƒï¼šé›†ä¸­åœ¨{min(times)}:00-{max(times)}:00"
        elif all(t < 12 for t in times):
            return "æ—¶é—´åˆ†å¸ƒï¼šä¸»è¦åœ¨ä¸Šåˆå‘å¸ƒ"
        elif all(t >= 12 for t in times):
            return "æ—¶é—´åˆ†å¸ƒï¼šä¸»è¦åœ¨ä¸‹åˆ/æ™šä¸Šå‘å¸ƒ"
        else:
            return f"æ—¶é—´åˆ†å¸ƒï¼šä»{min(times)}:00åˆ°{max(times)}:00ï¼Œåˆ†å¸ƒè¾ƒå¹¿"
    
    def create_intelligent_summary(self, posts: List[Dict], date: str) -> str:
        """ç”Ÿæˆæ™ºèƒ½å°ç»“"""
        
        # åŸºç¡€ç»Ÿè®¡
        post_count = len(posts)
        total_likes = sum(post.get('likes_count', 0) for post in posts)
        total_reposts = sum(post.get('reposts_count', 0) for post in posts)
        total_comments = sum(post.get('comments_count', 0) for post in posts)
        
        # åˆ†æç»„ä»¶
        keywords = self.extract_keywords(posts)
        sentiment = self.analyze_sentiment(posts)
        mentions = self.extract_mentions(posts)
        categories = self.categorize_content(posts)
        time_pattern = self.generate_time_analysis(posts)
        
        # æ„å»ºå°ç»“
        summary_parts = [
            f"## {date} Trump Truth Social åŠ¨æ€å°ç»“",
            "",
            f"**ğŸ“Š æ•°æ®æ¦‚è§ˆ**ï¼š{date}å…±å‘å¸ƒ{post_count}æ¡å¸–å­ï¼Œè·å¾—{total_likes:,}ä¸ªèµã€{total_reposts:,}æ¬¡è½¬å‘ã€{total_comments:,}æ¡è¯„è®ºã€‚",
            "",
            f"**â° {time_pattern}**",
            "",
            f"**ğŸ¯ å†…å®¹æƒ…æ„Ÿ**ï¼š{sentiment}",
            ""
        ]
        
        # ä¸»è¦è¯é¢˜
        if keywords:
            topic_list = []
            topic_map = {
                'election': 'é€‰ä¸¾æ”¿æ²»',
                'economy': 'ç»æµè®®é¢˜', 
                'border': 'è¾¹å¢ƒå®‰å…¨',
                'media': 'åª’ä½“æ‰¹è¯„',
                'political': 'æ”¿æ²»ç«‹åœº',
                'legal': 'æ³•å¾‹äº‹åŠ¡',
                'campaign': 'ç«é€‰æ´»åŠ¨'
            }
            
            for topic, count in sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:3]:
                chinese_topic = topic_map.get(topic, topic)
                topic_list.append(f"{chinese_topic}({count}æ¬¡)")
            
            summary_parts.extend([
                f"**ğŸ”¥ ä¸»è¦è¯é¢˜**ï¼š{' | '.join(topic_list)}",
                ""
            ])
        
        # å†…å®¹åˆ†ç±»
        if categories:
            summary_parts.append("**ğŸ“ å†…å®¹åˆ†ç±»**ï¼š")
            for category, items in categories.items():
                if items:
                    summary_parts.append(f"â€¢ **{category}** ({len(items)}æ¡)")
                    for item in items[:2]:  # æœ€å¤šæ˜¾ç¤º2æ¡
                        summary_parts.append(f"  - {item}")
            summary_parts.append("")
        
        # é‡è¦æåŠ
        if mentions:
            summary_parts.extend([
                f"**ğŸ‘¥ é‡è¦æåŠ**ï¼š{' | '.join(mentions)}",
                ""
            ])
        
        # åŸæ–‡é“¾æ¥
        summary_parts.extend([
            "### ğŸ“ åŸæ–‡é“¾æ¥ï¼š",
            *[f"â€¢ [{post.get('post_time', 'æ—¶é—´æœªçŸ¥')}] {post.get('post_url', '')}" 
              for post in sorted(posts, key=lambda x: x.get('post_time', ''))],
            "",
            "*æœ¬å°ç»“ç”±æœ¬åœ°æ™ºèƒ½ç®—æ³•ç”Ÿæˆï¼ŒåŸºäºå…³é”®è¯åˆ†æã€æƒ…æ„Ÿè¯†åˆ«å’Œå†…å®¹åˆ†ç±»æŠ€æœ¯ã€‚*"
        ])
        
        return "\n".join(summary_parts)
    
    def generate_daily_summary(self, date: str) -> Optional[str]:
        """ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„æ™ºèƒ½å°ç»“"""
        try:
            logger.info(f"å¼€å§‹ç”Ÿæˆ {date} çš„æœ¬åœ°æ™ºèƒ½å°ç»“")
            
            # è·å–æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰å¸–å­
            posts = self.db.get_posts_by_date(date)
            
            if not posts:
                logger.info(f"{date} æ²¡æœ‰å¸–å­æ•°æ®")
                return None
            
            logger.info(f"æ‰¾åˆ° {len(posts)} æ¡å¸–å­ï¼Œå¼€å§‹ç”Ÿæˆæ™ºèƒ½å°ç»“")
            
            # ç”Ÿæˆæ™ºèƒ½å°ç»“
            summary = self.create_intelligent_summary(posts, date)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            post_count = len(posts)
            total_likes = sum(post.get('likes_count', 0) for post in posts)
            total_reposts = sum(post.get('reposts_count', 0) for post in posts)
            total_comments = sum(post.get('comments_count', 0) for post in posts)
            
            summary_data = {
                'summary_date': date,
                'summary_content': summary,
                'post_count': post_count,
                'total_likes': total_likes,
                'total_reposts': total_reposts,
                'total_comments': total_comments,
                'generated_by': 'Local_AI'
            }
            
            if self.db.insert_daily_summary(summary_data):
                logger.info(f"{date} æœ¬åœ°æ™ºèƒ½å°ç»“ä¿å­˜æˆåŠŸ")
            
            return summary
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœ¬åœ°æ™ºèƒ½å°ç»“å¤±è´¥: {e}")
            return None


def main():
    """æµ‹è¯•æœ¬åœ°æ™ºèƒ½å°ç»“åŠŸèƒ½"""
    print("ğŸ¤– æœ¬åœ°æ™ºèƒ½å°ç»“ç³»ç»Ÿæµ‹è¯•")
    print("=" * 50)
    
    try:
        summarizer = LocalTrumpSummarizer()
        
        # ç”Ÿæˆæ˜¨å¤©çš„å°ç»“
        yesterday = (datetime.now(pytz.timezone(TIMEZONE)) - 
                    timedelta(days=1)).strftime('%Y-%m-%d')
        
        print(f"ğŸ“ æ­£åœ¨ç”Ÿæˆ {yesterday} çš„æœ¬åœ°æ™ºèƒ½å°ç»“...")
        summary = summarizer.generate_daily_summary(yesterday)
        
        if summary:
            print(f"\nâœ… æœ¬åœ°æ™ºèƒ½å°ç»“ç”ŸæˆæˆåŠŸ!")
            print(f"\n{summary}")
        else:
            print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ° {yesterday} çš„æ•°æ®")
            
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main() 
import time
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import json
import pytz
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import (
    TimeoutException, NoSuchElementException
)
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import random

from config import (
    TRUTH_SOCIAL_URL, BROWSER_OPTIONS, SCROLL_PAUSE_TIME,
    MAX_SCROLL_ATTEMPTS, MAX_NO_NEW_POSTS, TIMEZONE, MAX_RETRIES, RETRY_DELAY, USER_AGENT
)
from database import TrumpPostsDB
from utils import setup_logging

logger = setup_logging()


class TruthSocialScraper:
    """Truth Social Trumpå¸–å­çˆ¬è™«"""
    
    def __init__(self):
        self.db = TrumpPostsDB()
        self.driver = None
        self.et_tz = pytz.timezone(TIMEZONE)
    
    def human_like_scroll(self, pause_range=(3, 6)):
        """æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º - åŒ¿åè®¿é—®ä¼˜åŒ–ç‰ˆ"""
        # éšæœºæ»šåŠ¨è·ç¦»ï¼ˆè¾ƒå°ï¼Œæ¨¡æ‹ŸçœŸå®é˜…è¯»ï¼‰
        viewport_height = self.driver.execute_script("return window.innerHeight")
        scroll_distance = random.randint(
            int(viewport_height * 0.2), 
            int(viewport_height * 0.5)
        )
        
        # è·å–å½“å‰ä½ç½®
        current_pos = self.driver.execute_script("return window.pageYOffset")
        
        # åˆ†æ­¥æ»šåŠ¨ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
        steps = random.randint(2, 4)
        for i in range(steps):
            intermediate_pos = current_pos + (scroll_distance * (i + 1) / steps)
            self.driver.execute_script(f"window.scrollTo(0, {intermediate_pos});")
            time.sleep(random.uniform(0.2, 0.5))  # çŸ­æš‚åœé¡¿
        
        # éšæœºåœé¡¿ï¼Œæ¨¡æ‹Ÿé˜…è¯»
        pause_time = random.uniform(pause_range[0], pause_range[1])
        time.sleep(pause_time)
        
        return current_pos + scroll_distance
    
    def setup_driver(self):
        """è®¾ç½®æµè§ˆå™¨é©±åŠ¨ - å¢å¼ºåæ£€æµ‹ç‰ˆæœ¬"""
        try:
            chrome_options = Options()
            
            # åŸºç¡€æ— å¤´è®¾ç½®
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            
            # åæ£€æµ‹å¢å¼ºé€‰é¡¹
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·ä»£ç†
            chrome_options.add_argument(f"--user-agent={USER_AGENT}")
            
            # ç¦ç”¨ä¸€äº›å¯èƒ½æš´éœ²è‡ªåŠ¨åŒ–çš„åŠŸèƒ½
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-plugins")
            chrome_options.add_argument("--disable-images")  # åŠ é€ŸåŠ è½½
            
            try:
                # å°è¯•è‡ªåŠ¨ä¸‹è½½ChromeDriver
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                logger.info("è‡ªåŠ¨ä¸‹è½½ChromeDriveræˆåŠŸ")
            except Exception as e:
                logger.warning(f"è‡ªåŠ¨ä¸‹è½½ChromeDriverå¤±è´¥: {e}")
                logger.info("å°è¯•ä½¿ç”¨ç³»ç»ŸChrome...")
                self.driver = webdriver.Chrome(options=chrome_options)
            
            # è®¾ç½®è¶…æ—¶
            self.driver.set_page_load_timeout(60)
            self.driver.implicitly_wait(15)
            
            # åæ£€æµ‹è„šæœ¬
            self.driver.execute_script(
                "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            )
            
            logger.info("Chromeæµè§ˆå™¨é©±åŠ¨è®¾ç½®æˆåŠŸï¼ˆåŒ¿åè®¿é—® + åæ£€æµ‹ï¼‰")
            
        except Exception as e:
            logger.error(f"è®¾ç½®æµè§ˆå™¨é©±åŠ¨å¤±è´¥: {e}")
            raise
    
    def close_driver(self):
        """å…³é—­æµè§ˆå™¨é©±åŠ¨"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("æµè§ˆå™¨é©±åŠ¨å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­æµè§ˆå™¨é©±åŠ¨å¤±è´¥: {e}")
    
    def scroll_and_scrape_posts(
            self, target_date: Optional[str] = None
    ) -> List[Dict]:
        """è¾¹æ»šåŠ¨è¾¹å¤„ç†å¸–å­ï¼Œäººæ€§åŒ–åŒ¿åè®¿é—®ç‰ˆæœ¬"""
        scraped_posts = []
        processed_post_ids = set()
        scroll_attempts = 0
        no_new_posts_count = 0
        earliest_date_found = None
        target_reached = False
        
        logger.info(f"å¼€å§‹äººæ€§åŒ–åŒ¿åçˆ¬å–ï¼Œç›®æ ‡æ—¥æœŸ: {target_date}")
        
        # æ·»åŠ å®æ—¶è¿›åº¦æ˜¾ç¤º
        print(f"\nğŸš€ å¼€å§‹äººæ€§åŒ–åŒ¿åçˆ¬å– Truth Social å¸–å­...")
        if target_date:
            print(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
            print(f"ğŸ¯ åœæ­¢æ¡ä»¶: æ‰¾åˆ° {target_date} æˆ–æ›´æ—©çš„å¸–å­")
        print(f"ğŸŒ ä½¿ç”¨æ…¢é€Ÿäººæ€§åŒ–æ»šåŠ¨ï¼ˆåŒ¿åè®¿é—®ï¼‰")
        print(f"ğŸ”„ æœ€å¤§æ»šåŠ¨è½®æ¬¡: {MAX_SCROLL_ATTEMPTS} æ¬¡")
        print("=" * 60)
        
        try:
            while scroll_attempts < MAX_SCROLL_ATTEMPTS:
                # è·å–å½“å‰é¡µé¢ä¸Šçš„æ‰€æœ‰å¸–å­
                current_posts = self.driver.find_elements(
                    By.CSS_SELECTOR, ".status"
                )
                current_post_count = len(current_posts)
                
                # å®æ—¶æ˜¾ç¤ºè¿›åº¦
                progress_percent = (scroll_attempts + 1) / MAX_SCROLL_ATTEMPTS * 100
                earliest_info = f"æœ€æ—©: {earliest_date_found}" if earliest_date_found else "æœ€æ—©: æœªçŸ¥"
                print(f"\rğŸ“Š è¿›åº¦: [{scroll_attempts+1:2d}/{MAX_SCROLL_ATTEMPTS}] "
                      f"({progress_percent:5.1f}%) | "
                      f"é¡µé¢å¸–å­: {current_post_count:2d} | "
                      f"å·²å¤„ç†: {len(processed_post_ids):2d} | "
                      f"æ–°å¢: {len(scraped_posts):2d} | {earliest_info}", end="", flush=True)
                
                logger.info(
                    f"äººæ€§åŒ–æ»šåŠ¨è½®æ¬¡ {scroll_attempts + 1}: "
                    f"å½“å‰é¡µé¢å¸–å­æ•°é‡: {current_post_count}"
                )
                
                # å¤„ç†å½“å‰é¡µé¢ä¸Šçš„æ¯ä¸ªå¸–å­
                new_posts_found = 0
                oldest_post_date_this_round = None
                
                for post_element in current_posts:
                    try:
                        # å…ˆå°è¯•è·å–å¸–å­IDæ¥é¿å…é‡å¤å¤„ç†
                        try:
                            wrapper = post_element.find_element(
                                By.CSS_SELECTOR, ".status__wrapper"
                            )
                            post_id = wrapper.get_attribute("data-id")
                        except NoSuchElementException:
                            # å¦‚æœæ— æ³•è·å–IDï¼Œè·³è¿‡è¿™ä¸ªå¸–å­
                            continue
                        
                        # å¦‚æœå·²ç»å¤„ç†è¿‡è¿™ä¸ªå¸–å­ï¼Œè·³è¿‡
                        if post_id in processed_post_ids:
                            continue
                        
                        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨
                        if self.db.post_exists(post_id):
                            processed_post_ids.add(post_id)
                            logger.info(f"å¸–å­å·²å­˜åœ¨ï¼Œè·³è¿‡: {post_id}")
                            continue
                        
                        # æå–å¸–å­æ•°æ®
                        post_data = self.extract_post_data(post_element)
                        
                        if post_data and post_data.get('post_date'):
                            post_date = post_data['post_date']
                            
                            # æ›´æ–°æœ€æ—©å‘ç°çš„æ—¥æœŸ
                            if (oldest_post_date_this_round is None or 
                                    post_date < oldest_post_date_this_round):
                                oldest_post_date_this_round = post_date
                            
                            if (earliest_date_found is None or 
                                    post_date < earliest_date_found):
                                earliest_date_found = post_date
                            
                            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡æ—¥æœŸ
                            if target_date and post_date <= target_date:
                                target_reached = True
                                print(f"\nğŸ¯ æ‰¾åˆ°ç›®æ ‡æ—¥æœŸå¸–å­ï¼æ—¥æœŸ: {post_date}")
                                logger.info(f"æ‰¾åˆ°ç›®æ ‡æ—¥æœŸå¸–å­: {post_date} <= {target_date}")
                            
                            # ä¿å­˜åˆ°æ•°æ®åº“
                            if self.db.insert_post(post_data):
                                scraped_posts.append(post_data)
                                processed_post_ids.add(post_id)
                                new_posts_found += 1
                                logger.info(
                                    f"æˆåŠŸå¤„ç†å¸–å­: {post_id} (æ—¥æœŸ: {post_date})"
                                )
                                
                                # å®æ—¶æ˜¾ç¤ºæ–°å¸–å­
                                earliest_info = f"æœ€æ—©: {earliest_date_found}" if earliest_date_found else "æœ€æ—©: æœªçŸ¥"
                                print(f"\rğŸ“Š è¿›åº¦: [{scroll_attempts+1:2d}/{MAX_SCROLL_ATTEMPTS}] "
                                      f"({progress_percent:5.1f}%) | "
                                      f"é¡µé¢å¸–å­: {current_post_count:2d} | "
                                      f"å·²å¤„ç†: {len(processed_post_ids):2d} | "
                                      f"æ–°å¢: {len(scraped_posts):2d} âœ¨ | {earliest_info}", end="", flush=True)
                            else:
                                logger.error(f"ä¿å­˜å¸–å­å¤±è´¥: {post_id}")
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†å¸–å­æ—¶å‡ºé”™: {e}")
                        continue
                
                logger.info(
                    f"æœ¬è½®å¤„ç†äº† {new_posts_found} ä¸ªæ–°å¸–å­ï¼Œ"
                    f"æœ€è€å¸–å­æ—¥æœŸ: {oldest_post_date_this_round}, "
                    f"å…¨å±€æœ€æ—©æ—¥æœŸ: {earliest_date_found}"
                )
                
                # ä¼˜åŒ–çš„åœæ­¢æ¡ä»¶ï¼šåŸºäºæ—¥æœŸç›®æ ‡
                if target_date and target_reached:
                    # å·²ç»æ‰¾åˆ°ç›®æ ‡æ—¥æœŸçš„å¸–å­ï¼Œä½†å†æ»šåŠ¨å‡ è½®ç¡®ä¿å®Œæ•´æ€§
                    print(f"\nâœ… å·²æ‰¾åˆ°ç›®æ ‡æ—¥æœŸ {target_date} çš„å¸–å­ï¼Œå†æ»šåŠ¨å‡ è½®ç¡®ä¿å®Œæ•´æ€§...")
                    
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´æ—©çš„å¸–å­
                    additional_rounds = 3  # å‡å°‘é¢å¤–æ»šåŠ¨è½®æ¬¡
                    if scroll_attempts >= MAX_SCROLL_ATTEMPTS - additional_rounds:
                        print(f"ğŸ¯ ç›®æ ‡å·²è¾¾æˆï¼Œåœæ­¢çˆ¬å–")
                        logger.info(f"å·²åˆ°è¾¾ç›®æ ‡æ—¥æœŸ: {target_date} (å½“å‰æœ€æ—©: {earliest_date_found})")
                        break
                
                # æ‰§è¡Œäººæ€§åŒ–æ»šåŠ¨
                current_height = self.human_like_scroll(pause_range=(4, 8))
                
                # æ£€æŸ¥é¡µé¢å˜åŒ–å’Œåœæ­¢æ¡ä»¶
                updated_posts = self.driver.find_elements(By.CSS_SELECTOR, ".status")
                updated_post_count = len(updated_posts)
                
                # åŸºäºæ–°å¸–å­æ•°é‡çš„åœæ­¢æ£€æŸ¥
                if not target_reached:
                    if (new_posts_found == 0 and updated_post_count == current_post_count):
                        no_new_posts_count += 1
                        logger.info(f"æ²¡æœ‰æ–°å¸–å­ï¼Œè¿ç»­ {no_new_posts_count} æ¬¡")
                    else:
                        no_new_posts_count = 0
                    
                    # åŒ¿åè®¿é—®çš„æ¸©å’Œåœæ­¢ç­–ç•¥
                    if no_new_posts_count >= MAX_NO_NEW_POSTS:
                        if not target_date:
                            print(f"\nâš ï¸ è¿ç»­ {MAX_NO_NEW_POSTS} æ¬¡æ²¡æœ‰æ–°å¸–å­ï¼Œåœæ­¢çˆ¬å–")
                            logger.info("è¿ç»­å¤šæ¬¡æ²¡æœ‰æ–°å¸–å­ï¼Œåœæ­¢æ»šåŠ¨")
                            break
                        else:
                            print(f"\nâš ï¸ è¿ç»­ {MAX_NO_NEW_POSTS} æ¬¡æ²¡æœ‰æ–°å¸–å­ï¼Œä½†æœªè¾¾åˆ°ç›®æ ‡æ—¥æœŸ {target_date}")
                            print(f"ğŸ” å½“å‰æœ€æ—©æ—¥æœŸ: {earliest_date_found}")
                            
                            # æ‰§è¡Œæœ€åå°è¯•ï¼Œä½†æ›´æ¸©å’Œ
                            logger.info("æ‰§è¡Œæœ€åçš„æ¸©å’Œæ»šåŠ¨å°è¯•...")
                            self.human_like_scroll(pause_range=(6, 12))
                            
                            final_posts = self.driver.find_elements(By.CSS_SELECTOR, ".status")
                            if len(final_posts) == updated_post_count:
                                print("âŒ ç¡®è®¤å·²åˆ°è¾¾åŒ¿åè®¿é—®çš„æ•°æ®è¾¹ç•Œ")
                                logger.info("å·²åˆ°è¾¾åŒ¿åè®¿é—®çš„æ•°æ®è¾¹ç•Œ")
                                break
                            else:
                                new_count = len(final_posts) - updated_post_count
                                print(f"ğŸ‰ æ¸©å’Œæ»šåŠ¨åå‘ç° {new_count} ä¸ªæ–°å¸–å­ï¼Œç»§ç»­çˆ¬å–")
                                no_new_posts_count = 0
                
                scroll_attempts += 1
                logger.info(f"äººæ€§åŒ–æ»šåŠ¨è¿›åº¦: {scroll_attempts}/{MAX_SCROLL_ATTEMPTS}")
                
                # æ¯5è½®è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å°‘é¢‘ç‡ï¼‰
                if scroll_attempts % 5 == 0:
                    status = "ğŸ¯ å·²è¾¾æˆ" if target_reached else "ğŸ” æœç´¢ä¸­"
                    print(f"\nğŸ“ˆ é˜¶æ®µæ€»ç»“ - ç¬¬ {scroll_attempts} è½®:")
                    print(f"   ğŸ¯ å·²å¤„ç†å¸–å­: {len(processed_post_ids)} ä¸ª")
                    print(f"   âœ¨ æ–°å¢å¸–å­: {len(scraped_posts)} ä¸ª")
                    print(f"   ğŸ“… æœ€æ—©å¸–å­æ—¥æœŸ: {earliest_date_found or 'æœªçŸ¥'}")
                    if target_date:
                        print(f"   ğŸ¯ ç›®æ ‡æ—¥æœŸ: {target_date} ({status})")
                    print("=" * 60)
                    
                    # æ¨¡æ‹Ÿç”¨æˆ·ä¼‘æ¯
                    if scroll_attempts % 10 == 0:
                        print("ğŸ’­ æ¨¡æ‹Ÿç”¨æˆ·ä¼‘æ¯æ—¶é—´...")
                        time.sleep(random.uniform(8, 15))
            
            # æœ€ç»ˆç»“æœæ˜¾ç¤º
            print(f"\nğŸ‰ äººæ€§åŒ–åŒ¿åçˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            print(f"   ğŸ¯ å…±å¤„ç† {len(processed_post_ids)} ä¸ªå¸–å­")
            print(f"   âœ¨ æ–°å¢ {len(scraped_posts)} ä¸ªå¸–å­")
            print(f"   ğŸ“… æœ€æ—©å¸–å­æ—¥æœŸ: {earliest_date_found or 'æœªçŸ¥'}")
            if target_date:
                if target_reached:
                    print(f"   ğŸ¯ ç›®æ ‡æ—¥æœŸ {target_date}: âœ… å·²è¾¾æˆ")
                else:
                    print(f"   ğŸ¯ ç›®æ ‡æ—¥æœŸ {target_date}: âŒ æœªè¾¾æˆ")
                    print(f"   ğŸ’¡ å»ºè®®: åŒ¿åè®¿é—®çš„æ•°æ®æ·±åº¦é™åˆ¶çº¦ä¸º4å¤©")
            print("=" * 60)
            
            logger.info(
                f"äººæ€§åŒ–åŒ¿åçˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {len(processed_post_ids)} ä¸ªå¸–å­ï¼Œ"
                f"æ–°å¢ {len(scraped_posts)} ä¸ªï¼Œæœ€æ—©æ—¥æœŸ: {earliest_date_found}ï¼Œ"
                f"ç›®æ ‡è¾¾æˆ: {target_reached}"
            )
            return scraped_posts
            
        except Exception as e:
            print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            logger.error(f"æ»šåŠ¨å’Œçˆ¬å–è¿‡ç¨‹å¤±è´¥: {e}")
            return scraped_posts
    
    def _reached_target_date(self, target_date: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç»æ»šåŠ¨åˆ°ç›®æ ‡æ—¥æœŸ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # ä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨è·å–å¸–å­
            post_elements = self.driver.find_elements(
                By.CSS_SELECTOR, ".status"
            )
            
            if not post_elements:
                return False
            
            # æ£€æŸ¥æœ€åå‡ ä¸ªå¸–å­çš„æ—¶é—´
            for post in post_elements[-5:]:
                try:
                    # Truth Socialçš„æ—¶é—´åœ¨timeå…ƒç´ ä¸­
                    time_element = post.find_element(By.CSS_SELECTOR, "time")
                    title_time = time_element.get_attribute("title")
                    
                    if title_time:
                        # è§£ææ—¶é—´æ ¼å¼ "Jul 07, 2025, 10:24 AM"
                        try:
                            parsed_time = datetime.strptime(
                                title_time, "%b %d, %Y, %I:%M %p"
                            )
                            post_date_et = self.et_tz.localize(parsed_time)
                            post_date_str = post_date_et.strftime("%Y-%m-%d")
                            
                            if post_date_str <= target_date:
                                logger.info(
                                    f"æ‰¾åˆ°ç›®æ ‡æ—¥æœŸå¸–å­: {post_date_str} <= {target_date}"
                                )
                                return True
                                
                        except ValueError as e:
                            logger.debug(f"æ—¶é—´è§£æå¤±è´¥: {title_time}, {e}")
                            continue
                            
                except Exception as e:
                    logger.debug(f"è·å–å¸–å­æ—¶é—´å¤±è´¥: {e}")
                    continue
            
            return False
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç›®æ ‡æ—¥æœŸå¤±è´¥: {e}")
            return False
    
    def extract_post_data(self, post_element) -> Optional[Dict]:
        """ä»å¸–å­å…ƒç´ æå–æ•°æ®"""
        try:
            post_data = {}
            
            # è·å–å¸–å­HTML
            post_html = post_element.get_attribute("outerHTML")
            soup = BeautifulSoup(post_html, 'html.parser')
            
            # æå–å¸–å­ID
            try:
                # Truth Socialåœ¨.status__wrapperä¸­æœ‰data-idå±æ€§
                wrapper = post_element.find_element(By.CSS_SELECTOR, ".status__wrapper")
                post_id = wrapper.get_attribute("data-id")
            except NoSuchElementException:
                # å¦‚æœæ‰¾ä¸åˆ°wrapperï¼Œå°è¯•ä»é“¾æ¥ä¸­æå–ID
                try:
                    link_element = post_element.find_element(By.CSS_SELECTOR, "a[href*='/posts/']")
                    href = link_element.get_attribute("href")
                    post_id = href.split('/posts/')[-1].split('?')[0]
                except NoSuchElementException:
                    post_id = None
            
            if not post_id:
                logger.warning("æ— æ³•æå–å¸–å­ID")
                return None
            
            post_data['post_id'] = post_id
            
            # æå–å¸–å­å†…å®¹
            try:
                # Truth Socialä½¿ç”¨.status-contentç±»æ¥åŒ…å«å¸–å­å†…å®¹
                content_element = post_element.find_element(By.CSS_SELECTOR, ".status-content, .status__content")
                content = content_element.text.strip()
                post_data['content'] = content
            except NoSuchElementException:
                # å¦‚æœæ‰¾ä¸åˆ°æ ‡å‡†å†…å®¹å…ƒç´ ï¼Œå°è¯•è·å–æ•´ä¸ªå¸–å­çš„æ–‡æœ¬
                try:
                    content = post_element.text.strip()
                    # ç§»é™¤ç”¨æˆ·åå’Œæ—¶é—´æˆ³éƒ¨åˆ†
                    lines = content.split('\n')
                    if len(lines) > 3:
                        content = '\n'.join(lines[3:])  # è·³è¿‡ç”¨æˆ·åã€@handleå’Œæ—¶é—´
                    post_data['content'] = content
                except Exception:
                    logger.warning(f"å¸–å­ {post_id} æ— æ³•æå–å†…å®¹")
                    return None
            
            # æå–æ—¶é—´ä¿¡æ¯
            try:
                time_element = post_element.find_element(By.CSS_SELECTOR, "time")
                # Truth Socialåœ¨titleå±æ€§ä¸­æœ‰å®Œæ•´çš„æ—¶é—´ä¿¡æ¯
                title_time = time_element.get_attribute("title")
                datetime_str = time_element.get_attribute("datetime")
                
                if title_time:
                    # è§£ætitleä¸­çš„æ—¶é—´ (æ ¼å¼: "Jul 07, 2025, 10:24 AM")
                    try:
                        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                        parsed_time = datetime.strptime(title_time, "%b %d, %Y, %I:%M %p")
                        # å‡è®¾æ—¶é—´æ˜¯ä¸œéƒ¨æ—¶é—´
                        post_datetime_et = self.et_tz.localize(parsed_time)
                        post_data['timestamp_et'] = post_datetime_et.isoformat()
                        post_data['post_date'] = post_datetime_et.strftime("%Y-%m-%d")
                        post_data['post_time'] = post_datetime_et.strftime("%H:%M:%S")
                        
                        # è½¬æ¢ä¸ºUTC
                        post_datetime_utc = post_datetime_et.astimezone(pytz.UTC)
                        post_data['timestamp_utc'] = post_datetime_utc.isoformat()
                        
                    except ValueError:
                        logger.warning(f"å¸–å­ {post_id} æ—¶é—´æ ¼å¼è§£æå¤±è´¥: {title_time}")
                        return None
                        
                elif datetime_str:
                    # å¤‡ç”¨ï¼šä½¿ç”¨datetimeå±æ€§
                    post_datetime_utc = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
                    post_data['timestamp_utc'] = post_datetime_utc.isoformat()
                    
                    post_datetime_et = post_datetime_utc.astimezone(self.et_tz)
                    post_data['timestamp_et'] = post_datetime_et.isoformat()
                    post_data['post_date'] = post_datetime_et.strftime("%Y-%m-%d")
                    post_data['post_time'] = post_datetime_et.strftime("%H:%M:%S")
                    
                else:
                    logger.warning(f"å¸–å­ {post_id} æ— æ³•æå–æ—¶é—´")
                    return None
                    
            except NoSuchElementException:
                logger.warning(f"å¸–å­ {post_id} æ— æ³•æ‰¾åˆ°æ—¶é—´å…ƒç´ ")
                return None
            
            # æå–äº’åŠ¨æ•°æ®
            try:
                likes_element = post_element.find_element(By.CSS_SELECTOR, "[data-testid='like-count']")
                post_data['likes_count'] = int(likes_element.text.replace(',', '')) if likes_element.text else 0
            except (NoSuchElementException, ValueError):
                post_data['likes_count'] = 0
            
            try:
                reposts_element = post_element.find_element(By.CSS_SELECTOR, "[data-testid='repost-count']")
                post_data['reposts_count'] = int(reposts_element.text.replace(',', '')) if reposts_element.text else 0
            except (NoSuchElementException, ValueError):
                post_data['reposts_count'] = 0
            
            try:
                comments_element = post_element.find_element(By.CSS_SELECTOR, "[data-testid='comment-count']")
                post_data['comments_count'] = int(comments_element.text.replace(',', '')) if comments_element.text else 0
            except (NoSuchElementException, ValueError):
                post_data['comments_count'] = 0
            
            # æå–åª’ä½“URL
            media_urls = []
            try:
                media_elements = post_element.find_elements(By.CSS_SELECTOR, "img, video")
                for media in media_elements:
                    src = media.get_attribute("src")
                    if src and src.startswith("http"):
                        media_urls.append(src)
            except Exception:
                pass
            
            post_data['media_urls'] = json.dumps(media_urls) if media_urls else None
            
            # æ„å»ºå¸–å­URL
            post_data['post_url'] = f"https://truthsocial.com/@realDonaldTrump/posts/{post_id}"
            
            return post_data
            
        except Exception as e:
            logger.error(f"æå–å¸–å­æ•°æ®å¤±è´¥: {e}")
            return None
    
    def scrape_posts(self, days_back: int = 0) -> List[Dict]:
        """çˆ¬å–å¸–å­æ•°æ®"""
        scraped_posts = []
        
        try:
            self.setup_driver()
            
            # è®¿é—®Truth Socialé¡µé¢
            logger.info(f"è®¿é—® {TRUTH_SOCIAL_URL}")
            self.driver.get(TRUTH_SOCIAL_URL)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 30).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".status"))
            )
            
            # å¦‚æœéœ€è¦çˆ¬å–å†å²æ•°æ®ï¼Œè®¡ç®—ç›®æ ‡æ—¥æœŸ
            target_date = None
            if days_back > 0:
                target_date = (datetime.now(self.et_tz) - timedelta(days=days_back)).strftime("%Y-%m-%d")
                logger.info(f"ç›®æ ‡çˆ¬å–æ—¥æœŸ: {target_date}")
            
            # è¾¹æ»šåŠ¨è¾¹å¤„ç†å¸–å­
            scraped_posts = self.scroll_and_scrape_posts(target_date)
            
            logger.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(scraped_posts)} ä¸ªæ–°å¸–å­")
            return scraped_posts
            
        except TimeoutException:
            logger.error("é¡µé¢åŠ è½½è¶…æ—¶")
            return scraped_posts
        except Exception as e:
            logger.error(f"çˆ¬å–è¿‡ç¨‹å¤±è´¥: {e}")
            return scraped_posts
        finally:
            self.close_driver()
    
    def run_with_retry(self, days_back: int = 0) -> List[Dict]:
        """å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å–"""
        for attempt in range(MAX_RETRIES):
            try:
                logger.info(f"å¼€å§‹ç¬¬ {attempt + 1} æ¬¡çˆ¬å–å°è¯•")
                result = self.scrape_posts(days_back)
                
                if result or attempt == MAX_RETRIES - 1:
                    return result
                
            except Exception as e:
                logger.error(f"ç¬¬ {attempt + 1} æ¬¡çˆ¬å–å¤±è´¥: {e}")
                
                if attempt < MAX_RETRIES - 1:
                    logger.info(f"ç­‰å¾… {RETRY_DELAY} ç§’åé‡è¯•...")
                    time.sleep(RETRY_DELAY)
        
        return [] 